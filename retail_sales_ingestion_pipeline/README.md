# ğŸ›ï¸ Project 01: Retail Sales Data Ingestion with PySpark & PostgreSQL ğŸš€

## ğŸ“ Project Overview

This project simulates a **retail sales analytics use case**, where transactional data from a storeâ€™s daily sales (in CSV format) is ingested using **Apache PySpark**, transformed, and then stored in a **PostgreSQL** database. The pipeline supports **batch ingestion**, making it suitable for nightly or periodic loads into a data warehouse or BI system.

The goal is to demonstrate a **real-world, production-ready data pipeline** as part of a broader Data Engineering portfolio.

---

## âš™ï¸ Tech Stack

- ğŸ Python 3.x
- ğŸ”¥ Apache PySpark
- ğŸ˜ PostgreSQL
- ğŸ› ï¸ VS Code
- ğŸ“ Git & GitHub

---

## ğŸ“‚ Folder Structure

retail_sales_ingestion_pipeline/
â”œâ”€â”€ .env                         # PostgreSQL credentials (used instead of ini)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ scanner_data.csv     # Your input CSV file
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ db_config.py             # Loads credentials from .env
â”‚   â”œâ”€â”€ ingest_sales.py          # Main batch ingestion script
â”‚   â””â”€â”€ transform_utils.py       # Transformation helper functions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ create_env_file.py           # Optional helper to generate .env
