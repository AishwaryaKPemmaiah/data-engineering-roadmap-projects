# ğŸ“š Learning Data Engineering

This repository tracks my learning journey in Data Engineering. It includes multiple hands-on projects designed to apply concepts like batch processing, data modeling, orchestration, and streaming in real-world scenarios.

## ğŸš€ Projects in this repo:

| #  | Project Name                 | Description                                           |
|----|------------------------------|-------------------------------------------------------|
| 1  | PySpark CSV to Postgres      | Read CSV with PySpark, clean/transform, write to DB  |
| 2  | Airflow ETL Pipeline         | Build and schedule ETL jobs with Apache Airflow      |
| 3  | Data Modeling in PostgreSQL  | Implement star schema and Slowly Changing Dimensions |
| 4  | Real-Time Streaming          | Stream data using Kafka and PySpark                  |

---

## ğŸ’¡ Goal

To learn and apply data engineering skills by building real-world mini-projects using open-source and cloud tools.

## ğŸ§° Tools & Tech

- PySpark
- PostgreSQL
- Apache Airflow
- Apache Kafka
- Docker
- Cloud Platforms (AWS, GCP, Azure)

---

## ğŸ“Œ Notes

- This is a learning-focused repo â€” perfect for beginners exploring the data engineering path.
- Contributions or suggestions are welcome!

---

# ğŸ›ï¸ Project 01: Retail Sales Data Ingestion with PySpark & PostgreSQL ğŸš€

## ğŸ“ Project Overview

This project simulates a **retail sales analytics use case**, where transactional data from a storeâ€™s daily sales (in CSV format) is ingested using **Apache PySpark**, transformed, and then stored in a **PostgreSQL** database. The pipeline supports **batch ingestion**, making it suitable for nightly or periodic loads into a data warehouse or BI system.

The goal is to demonstrate a **real-world, production-ready data pipeline** as part of a broader Data Engineering portfolio.

---

## âš™ï¸ Tech Stack

- ğŸ Python 3.x
- ğŸ”¥ Apache PySpark
- ğŸ˜ PostgreSQL
- ğŸ› ï¸ VS Code
- ğŸ“ Git & GitHub

---

## ğŸ“‚ Folder Structure

retail_sales_ingestion_pipeline/
â”œâ”€â”€ .env                         # PostgreSQL credentials (used instead of ini)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ scanner_data.csv     # Your input CSV file
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ db_config.py             # Loads credentials from .env
â”‚   â”œâ”€â”€ ingest_sales.py          # Main batch ingestion script
â”‚   â””â”€â”€ transform_utils.py       # Transformation helper functions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ create_env_file.py           # Optional helper to generate .env
